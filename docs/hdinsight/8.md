# 第 7 章使用 C# Streaming 构建 Mapper

Hadoop 的一个关键组件是用于处理数据的 MapReduce 框架。这个概念是处理数据的代码的执行被发送到计算节点，这使它成为分布式计算的一个例子。此工作分为执行特定任务的许多作业。

Mappers 的工作相当于 ETL 范例的提取组件。他们读取核心数据并从中提取关键信息，实际上对非结构化数据施加了结构。顺便说一句，“非结构化”一词有点用词不当，因为数据并非完全没有结构 - 否则解析几乎是不可能的。相反，数据没有像在关系数据库中那样正式应用于它的结构。在这个意义上，管道分隔的文本文件可以被认为是非结构化的。因此，例如，我们的源数据可能如下所示：

```
1995|Johns, Barry|The Long Road to Succintness|25879|Technical
1987|Smith, Bob|I fought the data and the data won|98756|Humour
1997|Johns, Barry|I said too little last time|105796|Fictions

```

人眼可能会猜测这个数据可能是图书馆目录以及每个领域是什么。然而，计算机没有这样的运气，因为它没有被告知数据的结构。这在某种程度上是 Mapper 的工作。可以告诉该文件是管道分隔的，并且将作为密钥的作者姓名和作为值的字数提取为&lt; Key，Value&gt;。对。所以，这个 Mapper 的输出看起来像这样：

```
[key] <Johns, Barry> [value] <25879>
[key] <Smith, Bob> [value] <98756>
[key] <Johns, Barry> [value] <105796>

```

Reducer 等同于 ETL 范例的转换组件。它的工作是处理提供的数据。这可能像聚类算法那样复杂，也可能像聚合一样简单（例如，在我们的例子中，通过键对 Value 求和），例如：

```
[key] <Johns, Barry> [value] <131675>
[key] <Smith, Bob> [value] <98756>

```

此过程还有其他组件，特别是在单个 Mapper 节点上执行 Reducer 任务的某些功能的 Combiners。还有 Partitioners 指定 Reducer 输出发送到 Reducers 处理的位置。有关详细信息，请参阅 [http://wiki.apache.org/hadoop/HadoopMapReduce](http://wiki.apache.org/hadoop/HadoopMapReduce) 上的官方文档。

可以用.NET 语言编写一些作业，稍后我们将对此进行探讨。

## 流媒体概述

Streaming 是 Hadoop 功能的核心部分，允许在逐行的基础上处理 HDFS 中的文件。 [&lt;sup&gt;[13]&lt;/sup&gt;](../Text/hdi-13.html#_ftn13) 处理被分配给 Mapper（如果需要，还有 Reducer），专门为练习编码。

该过程通常在 Mapper 逐行读取文件块的情况下运行，从每行（STDIN）获取输入数据，对其进行处理，并将其作为键/值对发送到 STDOUT。 Key 是直到第一个制表符的任何数据和后面的任何值。然后，Reducer 将使用 STDOUT 中的数据，并根据需要进行处理和显示。

## 用 C# 流式传输

流式传输的一个关键特性是它允许将 Java 以外的语言用作执行 Map 和 Reduce 任务的可执行文件。因此，C# 可执行文件可以在流式作业中用作 Mapper 和 Reducers。

使用 Console.ReadLine（）处理输入（来自 STDIN）和 Console.WriteLine（）来写输出（到 STDOUT），很容易实现 C# 程序来处理数据流。 [&lt;sup&gt;[14]&lt;/sup&gt;](../Text/hdi-13.html#_ftn14)

在这个例子中，编写了一个 C# 程序来处理作为 Mapper 的原始数据的预处理，并进一步处理由 Pig 和 Hive 等高级语言处理。

下面引用的代码可以从 [https://bitbucket.org/syncfusiontech/hdinsight-succinctly/downloads](https://bitbucket.org/syncfusiontech/hdinsight-succinctly/downloads) 下载为“Sentiment_v2.zip”。需要使用合适的开发工具（如 Visual Studio）来处理代码。

## 数据源

对于这个例子，数据源是 Westbury 实验室 Usenet 语料库，收集了来自 47,000 个组的 2800 万个匿名化 Usenet 帖子，涵盖 2005 年 10 月至 2011 年 1 月期间。 [&lt;sup&gt;[15]&lt;/sup&gt;](../Text/hdi-13.html#_ftn15) 这是由人类输入的自由格式的英文文本，并提供了一个相当大的（大约 35GB）数据来源分析。

根据这些数据，我们可能希望提取发布 Usenet 帖子的人的用户名，发布的大致日期和时间，以及消息内容的细分。

### 数据挑战

在摄取这些数据时面临许多具体挑战：

*   跨越多行的一个项目的数据
*   作者姓名的位置格式不一致
*   这些帖子经常包含来自其他帖子的大量引用文本
*   有些词在不增加洞察力的情况下构成了数据的重要部分

处理这些问题的方式更为深入，因为它们表明了处理非结构化数据时所面临的挑战类型。

### 数据跨越多条线

所提供的数据对流媒体提出了特殊的挑战：提供的数据中的文本被分成多行。逐行流式处理流程，因此有必要跨多行保留元数据。我们的数据样本中的示例如下所示：

| 数据样本 |
| H.Q.Blanderfleet 写道：我给他们打电话告诉他们发生了什么......然后问我怎么不能在这个新号码上获得宽带我首先收到了电子邮件？--- END.OF.DOCUMENT --- |

这将由 STDIN 读取如下：

| 线#  | 正文 |
| 1 | H.Q.Blanderfleet 写道： |
| 2 |  |
| 3 | 我给他们打电话告诉他们发生了什么......然后问我怎么不能 |
| 4 | 在这个新号码上获得宽带我首先收到了电子邮件？ |
| 5 |  |
| 6 | --- END.OF.DOCUMENT --- |

这意味着 Mapper 必须能够：

*   确定新的数据元素
*   在行读取中维护元数据
*   处理分解为 HDFS 上的块的数据

识别新的数据元素很简单，因为每个帖子都由一行固定的文字“--- END.OF.DOCUMENT ---”分隔。通过这种方式，Mapper 可以安全地假设找到该文本表示当前帖子的结尾。

通过在正常变量中的行读取之间保留元数据，并在识别出行结束时重置它们来满足第二个挑战。发布的元数据附加到每个 Sentiment 关键字。

第三个挑战是解决数据文件可能被 HDFS 上的文件分块分解的事实，这意味着行将在一个文件中过早结束并在另一个文件中启动 midblock，如下所示：

| 档案 | 线#  | 正文 |
| A | 1 | H.Q.Blanderfleet 写道： |
| A | 2 |  |
| A | 3 | 我给他们打电话告诉他们发生了什么......然后问我怎么不能 |
| 文件分割 |
| B | 4 | 在这个新号码上获得宽带我首先收到了电子邮件？ |
| B | 5 |  |
| B | 6 | --- END.OF.DOCUMENT --- |

这是以简单的方式处理的。当文件 A 终止时，它会发出它到目前为止收集的所有数据。文件 B 只会将这些第一行丢弃为无效，因为它无法将元数据附加到它们。这是一种折衷方案，会导致数据丢失。

### 格式不一致

在档案库中，大多数消息以可变数量的空行开头，后面是开放行，有时但并非总是指示帖子的作者。这通常可以通过“[Username]写道：”模式来识别。

但是，这并不一致，因为各种 Usenet 客户端允许更改，不遵循标准格式或有时提取过程丢弃某些细节。开放线的一些例子如下：

| 开场线 | 评论 |
| “BasketCase”&lt; &LT; EMAILADDRESS&GT; &GT;在消息中写道&lt; NEWSURL&gt; ...... | 正如预期的那样，第一行在“写入”之前保留了一些文本。 |
| &gt; 2005 年 9 月 28 日星期三 02:13:52 -0400，East Coast Buttered&gt; &LT; &LT; EMAILADDRESS&GT; &GT;写道：&gt; | 第一行文字不包含“已写入”一词 - 它已被推到第二行。 |
| 曾几何时......很久很久以前......我决定拿到我家的电话号码改变了......因为我甚至从傻瓜那里得到了很多愚蠢的电话 | 该文本不包含作者详细信息。 |
| 2005 年 9 月 29 日星期四 13:15:30 +0000（UTC），“Foobar”写道： | 作者姓名前面有日期/时间戳。 |
| “Anonnymouse”&lt; &LT; EMAILADDRESS&GT; &GT;建议：&lt; NEWSURL&gt; ...... | 海报已从默认的“写入”改为“提议”。 |

作为最初的妥协，Mapper 完全忽略了所有非标准情况并将其标记为具有“未知”作者。

在一个改进中，常规表达式用于匹配一些更常见的日期戳格式并删除它们。代码示例中捕获了详细信息。

### 引用文字

在 Usenet 帖子中，许多客户端的默认行为是将先前消息包含为引用文本。为了分析给定消息的情感，需要将该文本排除在外，因为它来自另一个人。

幸运的是，这个引用的文本很容易被识别为带引号的行以“&gt;”开头，因此 Mapper 只是放弃了从该字符开始的任何行。这可能导致小但可容忍的数据丢失（如果帖子的作者有一条有意或无意地以“&gt;”字符开头的行）。这实现如下：

```
// Read line by line from STDIN
while ((line = Console.ReadLine()) != null)
{
     // Remove any quoted posts as identified by starting with ">"
     if (line.StartsWith(">") != true)
     {  
                //There is no “>” so process data

     }
…   
}

```

### 没有价值的词

英语中的一些单词非常常见，并且不会为简单的，基于单词的情感分析添加任何洞察力。

初次传递数据后，很明显有大量的双字母单词（“of”和“at”）和单个字符（“a”和“i”）可以忽略，特别是考虑到我们的 Sentiment 关键字列表没有长度少于三个字符的单词的条目。因此，应用了一个过滤器，阻止显示长度小于三个字符的任何字符串：

```
// Only write to STDOUT if there is content, ignoring words of 2 characters or less
if (descword.Length > 2)
{
Console.WriteLine("{0}", MessageId + "|" + AuthorId + "|" + descword);
}                         

```

此外，有一些非常高频率的词没有价值，如“和”和“the”。我们使用 Dictionary 和 lookup 来阻止这些显示，将上面的代码修改为：

```
// Set up some words to ignore
Dictionary<string, int> IgnoreWords = new Dictionary<string, int>();
IgnoreWords.Add("the", 1);
IgnoreWords.Add("and", 1);

 // Only write to STDOUT if there is content, ignoring words of 2 characters or less
if (descword.Length > 2)
{
     // Check if in list of ignore words, only write if not
     if (!IgnoreWords.TryGetValue(descword, out value))
     {
           Console.WriteLine(string.Format("{0}|{1}|{2}", MessageId ,AuthorId, descword));
     }                         
}

```

这显着减少了要显示的行数，因此随后进行了处理。

## 对数据样本执行 Mapper

提供的原始数据采用 bzip2 格式。 [&lt;sup&gt;[16]&lt;/sup&gt;](../Text/hdi-13.html#_ftn16) Hadoop 作业可以原生处理某些压缩文件格式，并以压缩格式显示结果如果指示。这意味着，为了执行样本，不需要解压缩来处理数据。 HDInsight 确认支持以下格式：

| 格式 | 编解码器 | 扩展 | 可拆分 |
| DEFLATE | org.apache.hadoop.io.compress.DefaultCodec | .deflate | N |
| gzip | org.apache.hadoop.io.compress.GzipCodec | .gz | N |
| bzip2 | org.apache.hadoop.io.compress.BZip2Codec | .bz2 | Y |

压缩输入和压缩输出的使用具有一些性能影响，需要根据存储和网络流量考虑进行平衡。有关 HDInsight 中这些注意事项的完整评论，建议您阅读 Microsoft 关于题为“Compression in Hadoop”（从中获取上表中的信息）主题的白皮书。 [&lt;sup&gt;[17]&lt;/sup&gt;](../Text/hdi-13.html#_ftn17)

Mapper 是作为标准 C# 控制台应用程序可执行文件构建的。为了使 Hadoop 作业能够使用它，需要将其加载到作业可以引用该文件的某个位置。 Azure Blob 存储是一个非常方便的处理方式。

加载数据和 Mapper 后，Hadoop 命令行用于指定和启动作业。也可以通过 SDK 或 PowerShell 提交作业。

该职位的完整语法如下：

```
c:\apps\dist\hadoop-1.1.0-SNAPSHOT\bin\hadoop.cmd
jar C:\apps\dist\hadoop-1.1.0-SNAPSHOT\lib\hadoop-streaming.jar
"-D mapred.output.compress=true"
"-D mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec"
-files "wasb://user/hadoop/code/Sentiment_v2.exe"
-numReduceTasks 0
-mapper "Sentiment_v2.exe"
-input "wasb://user/hadoop/data"
-output "wasb://user/hadoop/output/Sentiment/"

```

作业参数说明如下 [&lt;sup&gt;[18]&lt;/sup&gt;](../Text/hdi-13.html#_ftn18) ：

| 参数 | 详情 |
| “-D mapred.output.compress = true” | 压缩输出 |
| “-D mapred.output.compression.codec = org.apache.hadoop.io.compress.GzipCodec” | 使用 GzipCodec 压缩输出 |
| -files“wasb：//user/hadoop/code/Sentiment_v2.exe” | 在 Azure Blob 存储中引用 Mapper 代码 |
| -numReduceTasks 0 | 指定没有 Reducer 任务 |
| -mapper“Sentiment_v2.exe” | 指定 Mapper 的文件 |
| -input“wasb：// user / hadoop / data” | 指定输入目录 |
| -output“wasb：// user / hadoop / output / Sentiment /” | 指定输出目录 |

工作结果样本如下：

```
276.0|5|bob|government
276.0|5|bob|telling   
276.0|5|bob|oppose
276.0|5|bob|liberty
276.0|5|bob|obviously
276.0|5|bob|fail
276.0|5|bob|comprehend
276.0|5|bob|qualifier
276.0|5|bob|legalized
276.0|5|bob|curtis

```